{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff610d4b",
   "metadata": {},
   "source": [
    "### OCI Data Science - Useful Tips\n",
    "<details>\n",
    "<summary><font size=\"2\">Check for Public Internet Access</font></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "response = requests.get(\"https://oracle.com\")\n",
    "assert response.status_code==200, \"Internet connection failed\"\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Helpful Documentation </font></summary>\n",
    "<ul><li><a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Data Science Service Documentation</a></li>\n",
    "<li><a href=\"https://docs.cloud.oracle.com/iaas/tools/ads-sdk/latest/index.html\">ADS documentation</a></li>\n",
    "</ul>\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Typical Cell Imports and Settings for ADS</font></summary>\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "import ads\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.automl.provider import OracleAutoMLProvider\n",
    "from ads.automl.driver import AutoML\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from ads.common.data import ADSData\n",
    "from ads.explanations.explainer import ADSExplainer\n",
    "from ads.explanations.mlx_global_explainer import MLXGlobalExplainer\n",
    "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Useful Environment Variables</font></summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "print(os.environ[\"PROJECT_OCID\"])\n",
    "print(os.environ[\"USER_OCID\"])\n",
    "print(os.environ[\"TENANCY_OCID\"])\n",
    "print(os.environ[\"NB_REGION\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6fe892",
   "metadata": {},
   "source": [
    "DataLoader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f80ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b2c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_name, root_dir, training_length, forecast_window):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file.\n",
    "            root_dir (string): Directory\n",
    "        \"\"\"\n",
    "\n",
    "        # load raw data file\n",
    "#         csv_file = os.path.join(root_dir, csv_name)\n",
    "        self.df = pd.read_csv(csv_name)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = MinMaxScaler()\n",
    "        self.T = training_length\n",
    "        self.S = forecast_window\n",
    "        self.sessions = self.df['M_SESSION_UID'].unique().tolist()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # return number of sensors\n",
    "        return len(self.df.groupby(by=['M_SESSION_UID']))\n",
    "\n",
    "    # Will pull an index between 0 and __len__.\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # np.random.seed(0)\n",
    "\n",
    "        start = np.random.randint(0, len(self.df[self.df['M_SESSION_UID'] == self.sessions[idx]]) - self.T - self.S)\n",
    "        # sensor_number = str(self.df[self.df[self.df['M_SESSION_UID'] == self.sessions[idx]][[\"sensor_id\"]][start:start + 1].values.item())\n",
    "        index_in = torch.tensor([i for i in range(start, start + self.T)])\n",
    "        index_tar = torch.tensor([i for i in range(start + self.T, start + self.T + self.S)])\n",
    "        _input = torch.tensor(self.df[self.df['M_SESSION_UID'] == self.sessions[idx]][\n",
    "                                  [\"M_RAIN_PERCENTAGE\", \"sin_hour\", \"cos_hour\", \"sin_day\", \"cos_day\", \"sin_month\", \"cos_month\"]][\n",
    "                              start: start + self.T].values)\n",
    "        target = torch.tensor(self.df[self.df['M_SESSION_UID'] == self.sessions[idx]][\n",
    "                                  [\"M_RAIN_PERCENTAGE\", \"sin_hour\", \"cos_hour\", \"sin_day\", \"cos_day\", \"sin_month\", \"cos_month\"]][\n",
    "                              start + self.T: start + self.T + self.S].values)\n",
    "\n",
    "        # scalar is fit only to the input, to avoid the scaled values \"leaking\" information about the target range.\n",
    "        # scalar is fit only for humidity, as the timestamps are already scaled\n",
    "        # scalar input/output of shape: [n_samples, n_features].\n",
    "        scaler = self.transform\n",
    "\n",
    "        scaler.fit(_input[:, 0].unsqueeze(-1))\n",
    "        _input[:, 0] = torch.tensor(scaler.transform(_input[:, 0].unsqueeze(-1)).squeeze(-1))\n",
    "        target[:, 0] = torch.tensor(scaler.transform(target[:, 0].unsqueeze(-1)).squeeze(-1))\n",
    "\n",
    "        # save the scalar to be used later when inverse translating the data for plotting.\n",
    "        dump(scaler, 'scalar_item.joblib')\n",
    "\n",
    "        return index_in, index_tar, _input, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f897c1a4",
   "metadata": {},
   "source": [
    "helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6abddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "# save train or validation loss\n",
    "def log_loss(loss_val : float, path_to_save_loss : str, train : bool = True):\n",
    "    if train:\n",
    "        file_name = \"train_loss.txt\"\n",
    "    else:\n",
    "        file_name = \"val_loss.txt\"\n",
    "\n",
    "    path_to_file = path_to_save_loss+file_name\n",
    "    os.makedirs(os.path.dirname(path_to_file), exist_ok=True)\n",
    "    with open(path_to_file, \"a\") as f:\n",
    "        f.write(str(loss_val)+\"\\n\")\n",
    "        f.close()\n",
    "\n",
    "# Exponential Moving Average, https://en.wikipedia.org/wiki/Moving_average\n",
    "def EMA(values, alpha=0.1):\n",
    "    ema_values = [values[0]]\n",
    "    for idx, item in enumerate(values[1:]):\n",
    "        ema_values.append(alpha*item + (1-alpha)*ema_values[idx])\n",
    "    return ema_values\n",
    "\n",
    "# Remove all files from previous executions and re-run the model.\n",
    "def clean_directory():\n",
    "\n",
    "    if os.path.exists('save_loss'):\n",
    "        shutil.rmtree('save_loss')\n",
    "    if os.path.exists('save_model'): \n",
    "        shutil.rmtree('save_model')\n",
    "    if os.path.exists('save_predictions'): \n",
    "        shutil.rmtree('save_predictions')\n",
    "    os.mkdir(\"save_loss\")\n",
    "    os.mkdir(\"save_model\")\n",
    "    os.mkdir(\"save_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba0930",
   "metadata": {},
   "source": [
    "model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c229f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch, math\n",
    "\"\"\"\n",
    "The architecture is based on the paper “Attention Is All You Need”. \n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.\n",
    "\"\"\"\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    # d_model : number of features\n",
    "    def __init__(self,feature_size=7,num_layers=3,dropout=0):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=7, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        self.decoder = nn.Linear(feature_size,1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, device):\n",
    "        \n",
    "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "        output = self.transformer_encoder(src,mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebda217",
   "metadata": {},
   "source": [
    "plot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def plot_loss(path_to_save, train=True):\n",
    "    plt.rcParams.update({'font.size': 10})\n",
    "    with open(path_to_save + \"/train_loss.txt\", 'r') as f:\n",
    "        loss_list = [float(line) for line in f.readlines()]\n",
    "    if train:\n",
    "        title = \"Train\"\n",
    "    else:\n",
    "        title = \"Validation\"\n",
    "    EMA_loss = EMA(loss_list)\n",
    "    plt.plot(loss_list, label = \"loss\")\n",
    "    plt.plot(EMA_loss, label=\"EMA loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(title+\"_loss\")\n",
    "    plt.savefig(path_to_save+f\"/{title}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_prediction(title, path_to_save, src, tgt, prediction, sensor_number, index_in, index_tar):\n",
    "\n",
    "    idx_scr = index_in[0, 1:].tolist()\n",
    "    idx_tgt = index_tar[0].tolist()\n",
    "    idx_pred = [i for i in range(idx_scr[0] +1, idx_tgt[-1])] #t2 - t61\n",
    "\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.rcParams.update({\"font.size\" : 16})\n",
    "\n",
    "    # connect with last elemenet in src\n",
    "    # tgt = np.append(src[-1], tgt.flatten())\n",
    "    # prediction = np.append(src[-1], prediction.flatten())\n",
    "\n",
    "    # plotting\n",
    "    plt.plot(idx_scr, src, '-', color = 'blue', label = 'Input', linewidth=2)\n",
    "    plt.plot(idx_tgt, tgt, '-', color = 'indigo', label = 'Target', linewidth=2)\n",
    "    plt.plot(idx_pred, prediction,'--', color = 'limegreen', label = 'Forecast', linewidth=2)\n",
    "\n",
    "    #formatting\n",
    "    plt.grid(b=True, which='major', linestyle = 'solid')\n",
    "    plt.minorticks_on()\n",
    "    plt.grid(b=True, which='minor', linestyle = 'dashed', alpha=0.5)\n",
    "    plt.xlabel(\"Time Elapsed\")\n",
    "    plt.ylabel(\"Humidity (%)\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Forecast from Sensor \" + str(sensor_number[0]))\n",
    "\n",
    "    # save\n",
    "    plt.savefig(path_to_save+f\"Prediction_{title}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_training(epoch, path_to_save, src, prediction, sensor_number, index_in, index_tar):\n",
    "\n",
    "    # idx_scr = index_in.tolist()[0]\n",
    "    # idx_tar = index_tar.tolist()[0]\n",
    "    # idx_pred = idx_scr.append(idx_tar.append([idx_tar[-1] + 1]))\n",
    "\n",
    "    idx_scr = [i for i in range(len(src))]\n",
    "    idx_pred = [i for i in range(1, len(prediction)+1)]\n",
    "\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.rcParams.update({\"font.size\" : 18})\n",
    "    plt.grid(b=True, which='major', linestyle = '-')\n",
    "    plt.grid(b=True, which='minor', linestyle = '--', alpha=0.5)\n",
    "    plt.minorticks_on()\n",
    "\n",
    "    plt.plot(idx_scr, src, 'o-.', color = 'blue', label = 'input sequence', linewidth=1)\n",
    "    plt.plot(idx_pred, prediction, 'o-.', color = 'limegreen', label = 'prediction sequence', linewidth=1)\n",
    "\n",
    "    plt.title(\"Teaching Forcing from Sensor \" + str(sensor_number[0]) + \", Epoch \" + str(epoch))\n",
    "    plt.xlabel(\"Time Elapsed\")\n",
    "    plt.ylabel(\"Humidity (%)\")\n",
    "    plt.legend()\n",
    "    plt.savefig(path_to_save+f\"/Epoch_{str(epoch)}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_training_3(epoch, path_to_save, src, sampled_src, prediction, index_in, index_tar):\n",
    "\n",
    "    # idx_scr = index_in.tolist()[0]\n",
    "    # idx_tar = index_tar.tolist()[0]\n",
    "    # idx_pred = idx_scr.append(idx_tar.append([idx_tar[-1] + 1]))\n",
    "\n",
    "    idx_scr = [i for i in range(len(src))]\n",
    "    idx_pred = [i for i in range(1, len(prediction)+1)]\n",
    "    idx_sampled_src = [i for i in range(len(sampled_src))]\n",
    "\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.rcParams.update({\"font.size\" : 18})\n",
    "    plt.grid(b=True, which='major', linestyle = '-')\n",
    "    plt.grid(b=True, which='minor', linestyle = '--', alpha=0.5)\n",
    "    plt.minorticks_on()\n",
    "\n",
    "    ## REMOVE DROPOUT FOR THIS PLOT TO APPEAR AS EXPECTED !! DROPOUT INTERFERES WITH HOW THE SAMPLED SOURCES ARE PLOTTED\n",
    "    plt.plot(idx_sampled_src, sampled_src, 'o-.', color='red', label = 'sampled source', linewidth=1, markersize=10)\n",
    "    plt.plot(idx_scr, src, 'o-.', color = 'blue', label = 'input sequence', linewidth=1)\n",
    "    plt.plot(idx_pred, prediction, 'o-.', color = 'limegreen', label = 'prediction sequence', linewidth=1)\n",
    "    plt.title(\"Rain probability \" + \", Epoch \" + str(epoch))\n",
    "    plt.xlabel(\"Time Elapsed\")\n",
    "    plt.ylabel(\"Rain probability\")\n",
    "    plt.legend()\n",
    "    plt.savefig(path_to_save+f\"/Epoch_{str(epoch)}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4326c03e",
   "metadata": {},
   "source": [
    "train_with_sampling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5a608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import time # debugging\n",
    "from joblib import load\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math, random\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s %(message)s\", datefmt=\"[%Y-%m-%d %H:%M:%S]\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def flip_from_probability(p):\n",
    "    return True if random.random() < p else False\n",
    "\n",
    "def transformer(dataloader, EPOCH, k, frequency, path_to_save_model, path_to_save_loss, path_to_save_predictions, device):\n",
    "\n",
    "    device = torch.device(device)\n",
    "\n",
    "    model = Transformer().double().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=200)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    best_model = \"\"\n",
    "    min_train_loss = float('inf')\n",
    "\n",
    "    for epoch in range(EPOCH + 1):\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "\n",
    "        ## TRAIN -- TEACHER FORCING\n",
    "        model.train()\n",
    "        for index_in, index_tar, _input, target in dataloader:\n",
    "        \n",
    "            # Shape of _input : [batch, input_length, feature]\n",
    "            # Desired input for model: [input_length, batch, feature]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            src = _input.permute(1,0,2).double().to(device)[:-1,:,:] # torch.Size([24, 1, 7])\n",
    "            target = _input.permute(1,0,2).double().to(device)[1:,:,:] # src shifted by 1.\n",
    "            sampled_src = src[:1, :, :] #t0 torch.Size([1, 1, 7])\n",
    "\n",
    "            for i in range(len(target)-1):\n",
    "\n",
    "                prediction = model(sampled_src, device) # torch.Size([1xw, 1, 1])\n",
    "                # for p1, p2 in zip(params, model.parameters()):\n",
    "                #     if p1.data.ne(p2.data).sum() > 0:\n",
    "                #         ic(False)\n",
    "                # ic(True)\n",
    "                # ic(i, sampled_src[:,:,0], prediction)\n",
    "                # time.sleep(1)\n",
    "                \"\"\"\n",
    "                # to update model at every step\n",
    "                # loss = criterion(prediction, target[:i+1,:,:1])\n",
    "                # loss.backward()\n",
    "                # optimizer.step()\n",
    "                \"\"\"\n",
    "\n",
    "                if i < 24: # One day, enough data to make inferences about cycles\n",
    "                    prob_true_val = True\n",
    "                else:\n",
    "                    ## coin flip\n",
    "                    v = k/(k+math.exp(epoch/k)) # probability of heads/tails depends on the epoch, evolves with time.\n",
    "                    prob_true_val = flip_from_probability(v) # starts with over 95 % probability of true val for each flip in epoch 0.\n",
    "                    ## if using true value as new value\n",
    "\n",
    "                if prob_true_val: # Using true value as next value\n",
    "                    sampled_src = torch.cat((sampled_src.detach(), src[i+1, :, :].unsqueeze(0).detach()))\n",
    "                else: ## using prediction as new value\n",
    "                    positional_encodings_new_val = src[i+1,:,1:].unsqueeze(0)\n",
    "                    predicted_humidity = torch.cat((prediction[-1,:,:].unsqueeze(0), positional_encodings_new_val), dim=2)\n",
    "                    sampled_src = torch.cat((sampled_src.detach(), predicted_humidity.detach()))\n",
    "            \n",
    "            \"\"\"To update model after each sequence\"\"\"\n",
    "            loss = criterion(target[:-1,:,0].unsqueeze(-1), prediction)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach().item()\n",
    "\n",
    "        if train_loss < min_train_loss:\n",
    "            torch.save(model.state_dict(), path_to_save_model + f\"best_train_{epoch}.pth\")\n",
    "            torch.save(optimizer.state_dict(), path_to_save_model + f\"optimizer_{epoch}.pth\")\n",
    "            min_train_loss = train_loss\n",
    "            best_model = f\"best_train_{epoch}.pth\"\n",
    "\n",
    "\n",
    "        if epoch % 10 == 0: # Plot 1-Step Predictions\n",
    "\n",
    "            logger.info(f\"Epoch: {epoch}, Training loss: {train_loss}\")\n",
    "            scaler = load('scalar_item.joblib')\n",
    "            sampled_src_humidity = scaler.inverse_transform(sampled_src[:,:,0].cpu()) #torch.Size([35, 1, 7])\n",
    "            src_humidity = scaler.inverse_transform(src[:,:,0].cpu()) #torch.Size([35, 1, 7])\n",
    "            target_humidity = scaler.inverse_transform(target[:,:,0].cpu()) #torch.Size([35, 1, 7])\n",
    "            prediction_humidity = scaler.inverse_transform(prediction[:,:,0].detach().cpu().numpy()) #torch.Size([35, 1, 7])\n",
    "            plot_training_3(epoch, path_to_save_predictions, src_humidity, sampled_src_humidity, prediction_humidity, index_in, index_tar)\n",
    "\n",
    "        train_loss /= len(dataloader)\n",
    "        log_loss(train_loss, path_to_save_loss, train=True)\n",
    "        \n",
    "    plot_loss(path_to_save_loss, train=True)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d9e8f",
   "metadata": {},
   "source": [
    "main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc0d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(\n",
    "    epoch: int = 1000,\n",
    "    k: int = 60,\n",
    "    batch_size: int = 1,\n",
    "    frequency: int = 100,\n",
    "    training_length = 48,\n",
    "    forecast_window = 24,\n",
    "    train_csv = \"./weather_processed.csv\",\n",
    "    test_csv = \"./weather_processed.csv\",\n",
    "    path_to_save_model = \"save_model/\",\n",
    "    path_to_save_loss = \"save_loss/\", \n",
    "    path_to_save_predictions = \"save_predictions/\", \n",
    "    device = \"cpu\"\n",
    "):\n",
    "\n",
    "    clean_directory()\n",
    "\n",
    "    train_dataset = SensorDataset(csv_name = train_csv, root_dir = \"./\", training_length = training_length, forecast_window = forecast_window)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "    test_dataset = SensorDataset(csv_name = test_csv, root_dir = \"./\", training_length = training_length, forecast_window = forecast_window)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    best_model = transformer(train_dataloader, epoch, k, frequency, path_to_save_model, path_to_save_loss, path_to_save_predictions, device)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3bd688",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=1000\n",
    "k=60\n",
    "batch_size=1\n",
    "frequency=100\n",
    "path_to_save_model=\"save_model/\"\n",
    "path_to_save_loss=\"save_loss/\"\n",
    "path_to_save_predictions=\"save_predictions/\"\n",
    "device=\"cpu\"\n",
    "\n",
    "\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ae8745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch110_p37_cpu_v1]",
   "language": "python",
   "name": "conda-env-pytorch110_p37_cpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
